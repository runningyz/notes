### 决策树

+ 信息熵：用比特衡量信息，不确定性越大，熵越大

+ 信息增益：熵的差值

+ ID3算法（信息增益） C4.5（信息增益比率） CART（gini index） （属性选择的方法不一样）

+ 连续属性需要离散化
+ 防止过拟合：剪枝
  + 先剪枝（类达到一定纯度 不往下建树）
  + 后剪枝（建完树 再剪枝）
+ 优缺点：
  + 小数据集有效
  + 连续变量处理的不好
  + 类别太多 错误增加

### 临近取样（K-Nearest Neighbor）KNN

+ 基于输入实例的学习 懒惰学习
+ 步骤：
  + 选择参数k
  + 计算新实例与所有实例的距离（很多 可以是欧式距离 余弦值 相关度 曼哈顿距离） 取k个最近的
  + 新实例的类别：少数服从多数  k个实例当中 x类的实例最多 则新实例属于x
+ 优缺点：
  + 对k敏感 增大k降噪
  + 算法复杂度高
  + 样本不均衡时 会对算法效果有影响
+ 改进版：
  + 考虑距离 k根据距离加上权重 越近权重越大

### SVM

+ 寻找超平面 使得边际margin最大
+ ![1542356614052](https://github.com/runningyz/notes/blob/master/pic/deep_learning_pic/svm_margin.png)
+ 超平面到一边的举例等于到另一边的举例
+ 线性可分，线性不可分 （线性：在平面上 就是指直线）举例（下图都是线性不可分）
+ ![1542356814178](https://github.com/runningyz/notes/blob/master/pic/deep_learning_pic/linear_seprable.png)
+ 凡是刚好贴在margin上的点 叫支持向量 （因为是它们构成了margin）
+ 优点：
  + 不太容易overfitting
  + 依赖于支持向量 即使去掉训练集中支持向量外的其它所有点 也能得到一样的点
  + 支持向量数目越少 更容易泛化
+ 线性不可分的解决步骤：
  + 低维空间中的点 映射 到高维空间
  + 在高维空间 用超平面 按照线性可分方式去分
+ 核方法：优化求解时 会有很多內积运算 提出用核方法 使得k（x,y）x和y在核函数中的运算等于他俩在低维空间的內积
+ 常见的kernel：
  + 多项式核函数
  + 高斯径向基核函数（图像分类）
  + S型核函数


